---
title: "W(ha/il)t's the (wo/ba)rd?"
thanks: >
  "What's the word?" or "Wilt's the bard?" (nonsense).
subtitle: "A cross-sectional analysis of English word pairs that satisfy the properties of the Split Decisions word puzzle"
author: "Oliver Daniel"
abstract: >
  From three English wordlist corpora, pairs of words are generated which might appear in the Split Decisions word puzzle as published by the New York Times, i.e., at least 5 characters in length and differing by exactly two consecutive characters. The corpora are contrasted among themselves, and then the lexical and phonological properties of those words which form pairs – and those which do not – are analyzed for causative insights. Such insights will be alluded to here once discovered.
date: "April 30, 2022"
output:
  bookdown::pdf_document2:
    toc: true
    toc_depth: 1
    number_sections: false
knit: (function(input, ...) {
    rmarkdown::render(
    input,
    output_file = "out/paper_5.pdf",
    envir = globalenv(),
    ...
    )
  })
geometry: "margin=1.5in"
bibliography: in/references.bib
link-citations: true
---
\par \textbf{Keywords}: crosswords, split decisions, computational linguistics

Code available on [GitHub (link)](https://github.com/oliver-daniel/inf_313_final_paper).
```{r setup, echo=FALSE}
knitr::opts_chunk$set(
    echo = FALSE, warning = FALSE, cache = TRUE,
    out.width = "100%", out.align = "center", dir
)
options(knitr.graphics.error = FALSE)
```

```{r libraries, include=FALSE}
if (!require(tidyverse)) {
    install.packages("tidyverse")
}
if (!require(kableExtra)) {
    install.packages("kableExtra")
}

library(tidyverse)
library(knitr) # TODO CITE
library(kableExtra) # TODO CITE
```

\tableofcontents

## Introduction
<!-- TODO: "in this paper..." -->
### About Split Decisions
A few times a year, the New York Times games department releases a new issue of a word puzzle called Split Decisions. Similar to crosswords, the game board consists of boxes arranged in sequence, one for each letter of a word running either left-to-right or top-to-bottom, intersecting with other words at right angles, as shown below in Figure \ref{fig:example-puzzle}. The Split Decisions website [@split-decisions] attributes the original puzzle format to the late George Bredehorn; after his death, the mantle was taken up by Fred Piscop, whose puzzles appear in the *Times* to this day.
```{r example-puzzle, out.width="60%", fig.cap="\\label{fig:example-puzzle}An example Split Decisions puzzle from Piscop's website."}
knitr::include_graphics("out/res/example-puzzle.png")
```

Evidently, instead of cryptic descriptions of correct answers, Split Decisions solvers are only given pairs of two-letter substrings for each word. The remaining boxes, each containing one letter as in a typical crossword, combine with either of these substrings to form an English word. For example, the top-rightmost word, spelled horizontally ("across"), could take a P in its singular empty box to form the words POX and PRY, or an F to form FOX and FRY, potentially among other combinations. However, in filling out the rest of the puzzle, it becomes evident that only the latter solution accommodates both the across word pair, and the down pair that intersects it. 

### Rules, Notation and Terminology
In order to analyze different aspects of the game, we must first lay out a few rules of how Split Decisions puzzles work in general, as well as some additional constraints we introduce for the purpose of study. Additionally, we lay out some notation and terminology we will use to succinctly describe different segments of a puzzle word.

For lack of a robust \LaTeX mechanism for creating authentic-looking Split Decisions clues, like in Figure \ref{fig:example-puzzle}, we will use simpler notation to represent word pairs. Firstly, we will largely use underscores to represent those boxes left unfilled by the puzzle setter, and some visual delineator (often vertical alignment or slashes) to represent the provided two-letter substrings. For certain purposes, we may use a full equation-level \LaTeX environment, like:

$${PL \choose GR} \_\ \_\ \_\ \_$$

But, the above pair may also more succintly be represented in-text by a form like (PL/GR)\_\_\_\_ when more appropriate.

A correct fill for the remaining boxes might be (PL/GR)EASE, creating both PLEASE and GREASE, or as __EASE when the two missing letters are inessential. For the remainder of this paper, such a pair of words will be referred to as a *word pair*, sharing a *common substring*[^1] and differing by a *split pair*. The common substring and split pair meet at *boundaries*: using the convention of left-to-right for English, we might describe the above diagram as possessing a *right boundary*, but lacking a left one (as the split begins the word). In the (PL/GR)EASE notation used above, this right boundary is represented by the concatenation of the right parenthesis with another letter. In general, the shorthands *prefix* and *suffix* will be used to refer to common fill before the left boundary and after the right, respectively. Returning to the above example once more, (PL/GR)EASE lacks a prefix and has a suffix of "EASE".

Note that each clued split pair might accept several different fills for the common substring, e.g., (PL/GR)UNGE (PLUNGE, GRUNGE); however, as with crosswords, only one solution will also satisfy all the crossing constraints produced by the rest of the puzzle. 

Although a letter may appear on both sides of a split, say, (IT/TA), the letter will *never* appear at the same index in both words, e.g., (IT/AT), as such a pair of words would only differ by a single letter instead of two. Also, although such a puzzle would be fascinating to both solve and analyze, crossing words do *not* go through split pairs, only common letters.

Finally, although a word pair can theoretically be as short as three letters, this paper examines only pairs of length five and above.

Upon understanding the rules of the puzzle, one may start to wonder: what sorts of words form split pairs? Are some splits more common than others? How do the properties of English, as both a spoken and written language, affect the guessability of a given split pair? In this paper, we perform preliminary statistical analysis to investigate these questions, using a variety of computational methods.


## Data
### Corpora: What's a "word"?
One of the most common side effects of continued study in linguistics is a diminished ability to describe what defines a "word". For the purpose of this study, we have taken three lists that purport to contain a wide variety of English words, though certainly not their totality, and used them to generate all possible combinations that produce a valid Split Decisions word pair (in terms of a two-character split pair, etc.). As a result, many entries that appear in one or more of these corpora may seem esoteric, or even offensive, to be cosnidered as words; however, the only predicate for their inclusion in these lists is attested usage, not whether they prescriptively *ought* to be a word. Words appear in these corpora in lexicographical order (i.e., sorted alphabetically), one on each line, in lower-case letters.

The first and smallest corpus comes from an open-source distribution of GNU/Linux, specifically Ubuntu. For brevity, I will usually refer to this corpus as `linux` (note the lower-case letters and monospace font). This corpus, developed over time by online contributors through the development of Ubuntu, is accessed on Linux machines through the symbolic link `/usr/share/dict/words`. It is primarily used as a baseline for other text processing programs' spell-checking and text prediction functionalities.

The `collins` corpus [@collins] is privately produced by HarperCollins LLC, a publishing company well known for producing English- and multiple-language dictionaries. This dictionary in particular finds notable use in competitive Scrabble play: competitors in so-called 'Collins divisions' at tournaments will study from and refer to this list as the superset of all playable Scrabble words.

Similarly, the `nwl` corpus [@nwl2020] was produced directly by members of the North American Scrabble Association (NASPA) for the purpose of tournament play. In Canada and the United States, this word list serves as the *de facto* standard for competitive Scrabble, and is usually the default wordlist unless another is specified, as in the aforementioned Collins divisions.

From each of these corpora, only those entries consisting solely of five or more Latin characters without diacritics were accepted for further processing.
```{r read-data}
corpus.linux <- read.table("in/corpus/linux.txt")
corpus.collins <- read.table("in/corpus/collins.txt")
corpus.nwl <- read.table("in/corpus/NWL2020.txt")

dat.linux <-
    read.csv("in/cache/linux.csv") |>
    tibble() |>
    mutate(
        corpus = "linux",
    )

dat.collins <-
    read.csv("in/cache/collins.csv") |>
    tibble() |>
    mutate(
        corpus = "collins",
        start = start + 1, # Made this column zero-indexed by accident :)
        end = end + 1
    )

dat.nwl <-
    read.csv("in/cache/NWL2020.csv") |>
    tibble() |>
    mutate(corpus = "nwl")

dat.all <- function() {
    rbind(
        dat.linux,
        dat.collins,
        dat.nwl
    )
}
```

```{r}
most_common <- function(col, n = 1) {
    table(col) |>
        sort() |>
        names() |>
        tail(n)
}

PATTERN <- "[a-z]{5,}"

corpus_stats <- rbind(
    tibble(
        corpus = "linux",
        all_words = corpus.linux |> nrow(),
        valid_words = corpus.linux |>
            filter(str_detect(V1, PATTERN)) |>
            nrow(),
        pairs_count = dat.linux |> nrow()
    ),
    tibble(
        corpus = "collins",
        all_words = corpus.collins |> nrow(),
        valid_words = corpus.collins |>
            filter(str_detect(V1, PATTERN)) |>
            nrow(),
        pairs_count = dat.collins |> nrow()
    ),
    tibble(
        corpus = "nwl",
        all_words = corpus.nwl |> nrow(),
        valid_words = corpus.nwl |>
            filter(str_detect(V1, PATTERN)) |>
            nrow(),
        pairs_count = dat.nwl |> nrow()
    )
)

split_pair_counts <- dat.all() |>
    mutate(split_pair = paste0(split_x, "/", split_y)) |>
    group_by(corpus) |>
    count(split_pair, name = "split_pair_count") |>
    slice_max(n = 1, order_by = split_pair_count, with_ties = F)


split_x_counts <- dat.all() |>
    group_by(corpus) |>
    count(split_x, name = "split_x_count") |>
    slice_max(n = 1, order_by = split_x_count, with_ties = F)

split_y_counts <- dat.all() |>
    group_by(corpus) |>
    count(split_y, name = "split_y_count") |>
    slice_max(n = 1, order_by = split_y_count, with_ties = F)
```

```{r corpus-stats}
corpus_stats |>
    left_join(split_x_counts, by = "corpus") |>
    left_join(split_y_counts, by = "corpus") |>
    left_join(split_pair_counts, by = "corpus") |>
    mutate(
        split_x = paste0(split_x, " (", split_x_count, ")"),
        split_y = paste0(split_y, " (", split_y_count, ")"),
        split_pair = paste0(split_pair, " (", split_pair_count, ")")
    ) |>
    select(
        corpus, all_words,
        valid_words, pairs_count, split_x,
        split_y, split_pair
    ) |>
    kable(
        booktabs = F,
        col.names = c(
            "Corpus", "Total",
            "Valid", "Total", "First split",
            "Second split", "Split pair"
        ),
        caption = "Post-processing statistics for each corpus of English words."
    ) |>
    kable_classic(latex_options = "hold_position") |>
    add_header_above(c(" " = 1, "Words" = 2, "Word pairs" = 1, "Most frequent (occurrences)" = 3))
```

### Processing & Caching
The algorithm for determining whether two strings of the same length form a Split Decisions pair is quite simple.

1. Traverse the indices of both strings, comparing their characters pairwise. That is, compare the first letters of both strings, then the second, and so on.
2. Record each index at which the two characters differ.
3. Once traversal is complete, if there are exactly two differences and they differ in index by exactly one, then the two strings form a word pair. Otherwise, continue with another combination of strings.

However, initial experiments with identifying these word pairs directly in R [@R] proved to be untenably slow. Since every word in a corpus has to be compared to every other word of the same length at least once, any algorithm[^2] used to do so cannot run in better than quadratic time, and even R packages with bindings to C took several hours to run on even the smallest of the corpora. Instead, a Python script was written to process the words more efficiently, and then write the results in a CSV file that could be more readily imported into the R environment. This precomputation phase significantly reduced start-up time when creating R sessions to author this paper, or to knit this document into a PDF using `knitr` [@knitr]. A link to this script can be found in the Appendix.

Apart from these, the remainder of data processing from this paper was performed in R using the popular `tidyverse` package collection [@tidyverse], and visualized using the indispensible `ggplot2` [@ggplot2].

### Variables
These cached CSVs accounted for the following variables of each word pair:

1. `length`: the length of each word in the pair. Ranges from 5 to 21.
2. `x`: the lexicographically-first full word in the pair, with no additional notation for the split or any boundaries.
3. `y`: the lexicographically-second full word in the pair.
4. `start`: the 1-index of the beginning of the split. For example, a split at the beginning of a word would have value 1.
5. `end`: The 1-index of the end of the split. This is a convenience variable: each split has a length of exactly two, and hence the ending index is always equal to `start + 1`.
6. `split_x`: The lexicographically-first of the two splits. As a property, the `start`th to `end`th substring of `x` is always this value.
7. `split_y`: The lexicographically-second of the two splits, as above.
8. `corpus`: The corpus from which this pair was determined. One of `linux`, `collins`, or `nwl`.

In certain context-specific scenarios, other variables are computed:

9. `type`: divides each word pair into one of three categories, depending on the positioning of its split pair. If the split pair is at the beginning of the word, the type is "prefix"; if at the end, "suffix"; and otherwise, "middle". (PL/GR)EASE is a "prefix"-type pair, for example.
10. `rank`: when comparing different pairs, splits, etc. by their frequency, a ranking variable is introduced to keep track of which is the most frequent, the second most frequent, and so on.
<!-- TODO -->

Figure \ref{fig:split-violins}, below, demonstrates the distribution of these different split types across word pairs of different corpora and lengths. For example, 15-letter word pairs from the `linux` corpus (in green) overwhelmingly favour suffix-type splits.
```{r split-violins, fig.cap="\\label{fig:split-violins}Width at a particular y-value represents the density of splits in word pairs of a given length, starting at that index."}
dat.all() |>
    filter(length <= 15) |>
    ggplot(aes(
        x = factor(length),
        y = start,
        color = corpus
    )) +
    geom_violin(scale = "width") +
    facet_wrap(~corpus, dir = "v") +
    scale_y_continuous(breaks = c(2, 4, 6, 8, 10, 12, 14, 16)) +
    labs(
        title = str_wrap("Proportional distribution of split pair indices across corpus and pair length", 60),
        x = "Length (in letters)",
        y = "Starting 1-index of splits",
    ) +
    theme(
        legend.position = "none"
    )
```

As discussed later in **Results**, we hypothesize that the goblet-like shape seen predominantly in longer words (i.e,., 9+ letters) is due to a widened variety of verbal stems that differ only in their declension, as indicated by a suffix (e.g., participating, participation).

Looking at all consecutive pairings of letters (*2-grams*) in each corpus, we see that 614 out of a possible $26^2 = 676$ pairings are accounted for at least once. Figure \ref{fig:bigram-heatmap} below depicts the relative frequency of these pairings, with the white squares representing those pairs that appear nowhere in the corpus (or corpora). Some more infrequent first letters, such as *q* or *z*, demonstrate large streaks of unattested pairs. This is in line with certain orthotactic[^3] constraints present in the English language and will be discussed in greater detail in **Discussion**.

```{r bigram-heatmap, fig.cap="\\label{fig:bigram-heatmap}Distributions appear relatively consistent across corpora, except for particularly esoteric combinations: the Linux corpus contains no q's followed by anything other than a u."}
corpus_2grams <- read.csv("in/cache/corpus_2grams.csv")
corpus_2grams |>
  ggplot(aes(
    y = substr(splits, 1, 1),
    x = substr(splits, 2, 2),
    fill = after_stat(count / max(count))
  )) +
  geom_bin2d() +
  geom_bin2d(data = transform(corpus_2grams, corpus = "total")) +
  coord_fixed() +
  scale_fill_continuous(type = "viridis") +
  facet_wrap(~corpus, ncol = 2) +
  labs(
      title = str_wrap("Proportional frequency of 2-grams across all words", 60),
      y = "First letter",
      x = "Second letter",
      fill = "Proportional frequency"
  ) +
  theme()
```

## Results
Upon encountering such a wide distribution of categorical data (i.e., ordered strings of characters), our first thought was to see if some aspect of the distribution obeyed Zipf's Law [@zipf]. In brief, this empirical law states that when observations in a categorical set are ranked by frequency, the second-ranked observation occurs roughly half as often as the first, the third-ranked a third as often, and so on. In mathematical terms, over a frequency-ordered distribution $X$ with ranking variable $i$,

$$\text{Zipf}(X) \iff X_i \approx \frac{X_1}{i}.$$

```{r letter_distributions}
make_letter_dist <- function(corpus) {
  read.table(paste0("in/corpus/", corpus, ".txt")) |>
    tibble() |>
    filter(str_detect(V1, "^[a-z]{5,}$")) |>
    pluck("V1") |>
    str_split("") |>
    unlist() |>
    table() |>
    bind_rows() |>
    pivot_longer(cols = 1:26) |>
    mutate(value = as.numeric(value))
}

letter_dists <- make_letter_dist("linux") |>
  mutate(corpus = "linux") |>
  rbind(make_letter_dist("collins") |>
    mutate(corpus = "collins")) |>
  rbind(make_letter_dist("NWL2020") |>
    mutate(corpus = "nwl"))

letter_rankings <- letter_dists |>
  pivot_wider(names_from = corpus) |>
  mutate(total = rowSums(across(2:4))) |>
  arrange(desc(total))
```

Before examining the splits and other puzzle-specific properties of words, we examined the distribution of one- and two-letter sequences to see if any Zipf-like patterns emerged. Surprisingly, however, the data outperformed Zipf's Law (i.e., contained many more observations than the power law predicts) in these categories. Figure \ref{fig:single-letter-distribution} below, which examines the total occurrences of single letters in all valid words, exemplifies this. The black dashed curve represents the expected count of each letter according to Zipf's Law, whereas the total height of each bar represents the actual count. The second-most common letter, *s*, is expected to occur $539,056 / 2 = 269,528$ times, or 50% of the total occurrences of *e*; it instead appears roughly 84% as often, a total of `r letter_rankings$total[[2]]` times. This disparity between Zipfian expectation and reality only increases in higher ranks, save for the universally rare *q* and *j*.

```{r single-letter-distribution, fig.cap="\\label{fig:single-letter-distribution}The total distribution of letters across all valid corpus words is consistently greater than the ideal Zipf curve (black) would predict."}


letter_dists |>
  rowwise() |>
  mutate(
    rank = match(name, letter_rankings$name),
    f = letter_rankings[["total"]][[rank]] / rank
  ) |>
  ggplot(aes(x = reorder(name, rank), y = value, fill = corpus)) +
  geom_bar(stat = "identity") +
  geom_line(aes(x = rank, y = f), linetype = 2) +
  labs(
      title = str_wrap("Distribution of letters in corpus words: Zipf's Law vs. observations", 60),
      x = "Letter",
      y = "Occurrences"
  ) +
  theme()
```

Looking at each corpus individually, we can also examine their 2-grams; that is, each consecutive pair of characters that occurs in every valid word. As an example, the word "aardvark" contains the 2-grams "aa", "ar", "rd", "dv", "va", "ar", and "rk". (Note that "ar" occurs twice, and is counted as such.) Figure \ref{fig:two-letter-distribution} below again pits the frequency of these 2-grams against the expectation of Zipf's Law, yielding an interesting result. For clarity and visual interest, some of these 2-grams have been included beside their place on the graph, serving as waypoints of increasingly uncommon letter pairings.

```{r two-letter-distribution, fig.cap="\\label{fig:two-letter-distribution}Looking at all 2-grams across the three corpora, distributions of all but the bottom ~190 attested instances outperform a Zipf curve."}

corpus_2grams <- read.csv("in/cache/corpus_2grams.csv")

labelled_points <- c(1, 3, 10, 100, 200, 300, 400, 500, 600, 616)
corpus_2grams |>
  count(corpus, splits) |>
  group_by(corpus) |>
  mutate(
    rank = rank(-n, ties.method = "first"),
    f = max(n) / rank,
    label = ifelse(rank %in% labelled_points, splits, NA)
  ) |>
  ggplot(aes(x = rank, y = n, color = corpus, label = label)) +
  geom_point() +
  geom_line(aes(y = f), linetype = 2) +
  geom_vline(aes(xintercept = 420), linetype = 3) +
  geom_text(hjust = -1.5, vjust = .5) +
  scale_x_continuous(limits = c(1, 26^2), breaks=c(1, 100, 200, 300, 400, 500, 616)) +
  scale_y_log10() +
  labs(
      title = str_wrap("Distribution of 2-grams in corpus words: Zipf's Law vs. observations", 60),
      x = "Rank of 2-gram by frequency",
      y = "Occurrences (log-scale)"
  ) +
  theme()
```



```{r split-letters-heatmap, fig.cap="\\label{fig:split-letters-heatmap}Split pairs are predominated by a few major players: es, ng, re, st, etc."}
all_split_pairs <- dat.all() |>
  select(split = split_x, corpus) |>
  rbind(dat.all() |> select(split = split_y, corpus))

all_split_pairs |>
  ggplot(aes(
    x = substr(split, 2, 2),
    y = substr(split, 1, 1),
    fill = after_stat(count) / max(count)
  )) +
  geom_bin2d() +
  geom_bin2d(data = transform(all_split_pairs, corpus = "total")) +
  coord_fixed() +
  scale_y_discrete(limits = rev(letters)) +
  scale_fill_continuous(type = "viridis") +
  facet_wrap(~corpus, ncol = 2) +
  labs(
      title = str_wrap("Proportional frequency of split 2-grams", 60),
      y = "First letter",
      x = "Second letter",
      fill = "Proportional frequency"
  ) +
  theme()
```

```{r top-splits-by-position}
splits_by_type <- dat.all() |>
  mutate(
    type = case_when(
      start == 1 ~ "prefix",
      end == length ~ "suffix",
      TRUE ~ "middle"
    )
  )

rbind(
  splits_by_type |> select(split = split_x, type),
  splits_by_type |> select(split = split_y, type)
) |>
  count(split, type, sort = T) |>
  group_by(type) |>
  do(head(., 5)) |>
  mutate(str_rep = paste0(split, " (", n, ")"), id = row_number()) |>
  pivot_wider(names_from = type, values_from = str_rep) |>
  group_by(id) |>
  summarise_all(~ na.omit(.)[1]) |>
  select(id, prefix, middle, suffix) |>
  kable(booktabs = F, col.names = c(
      "Rank", "prefix", "middle", "suffix"
  ), caption = "Most common splits by word position.") |>
  kable_classic(latex_options = "hold_position")
```

```{r split-position-distribution, }

rbind(
  splits_by_type |> select(split = split_x, type),
  splits_by_type |> select(split = split_y, type)
) |>
  count(split, type) |>
  group_by(type) |>
  mutate(
    rank = rank(-n, ties.method = "first"),
    f = max(n) / rank,
    label = ifelse(rank %in% labelled_points, split, NA)
  ) |>
  ggplot(aes(x = rank, y = n, color = type, label = label)) +
  geom_point() +
  geom_line(aes(x = rank, y = f), linetype = 2) +
  geom_text(vjust = -1.5, hjust = .5, size = 3) +
  # geom_smooth(method = "lm", se = F, alpha = .5) +
  scale_x_continuous(limits = c(1, 26^2)) +
  scale_y_log10() +
  labs(
      title = str_wrap("Distribution of splits by word position", 60),
      y = "Occurrences (log-scale)",
      x = "Frequency rank"
  )
```

## Discussion
### Orthotactics

### Phonotactics

### 

\newpage
## Appendix
<!-- TODO: python script -->
### Link to Python preprocessing script
Found in the [GitHub repo (link)](https://github.com/oliver-daniel/inf_313_final_paper/blob/main/scripts/find_pairs.py).

\newpage
## References




<!-- Footnotes -->
[^1]: Or *fill*: this part of the puzzle is filled in by the player rather than the constructor.
[^2]: i.e., any naive iterative algorithm.
[^3]: Pertaining to the rules and constraints of spelling. E.g., "pwn" violates English orthotactic rules, and thus could never constitute a non-abbreviated English word.